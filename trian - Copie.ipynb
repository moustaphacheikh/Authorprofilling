{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pydot_ng as pydot\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Bidirectional, GRU\n",
    "from keras.models import Sequential\n",
    "from utils import *\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.callbacks import *\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import MaxPooling1D, Conv1D\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import *\n",
    "from keras.utils import plot_model  \n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "x_train, y_train, x_test, y_test = load_ASTD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     962\n",
       "unique      2\n",
       "top         0\n",
       "freq      481\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTD dataset\n",
      "Tokenizing texts\n",
      "Saved tokenizer.pkl\n",
      "Found 7277 unique 1-gram tokens.\n",
      "Min sequence length: 0\n",
      "Average sequence length: 14\n",
      "Max sequence length: 29\n",
      "Finished tokenizing texts\n",
      "--------------------------------------------------------------------------------\n",
      "Load embedding model.\n",
      "Numbers of words in embedding model : 667062\n",
      "Preparing embedding matrix.\n",
      "Embeding matrix size : 7278\n",
      "Saved embedding matrix\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "max_num_words = 10000\n",
    "max_sequence_length = 30\n",
    "embedding_path = './data/embeddings/wiki.ar.vec'\n",
    "embedding_path = './data/embeddings/wiki_corpus_tweets_xtrain_word2vec_size300_skip_gram_wv_iter=10'\n",
    "gensim = True\n",
    "print('ASTD dataset')\n",
    "x_train, y_train, x_test, y_test = load_ASTD()\n",
    "print('Tokenizing texts')\n",
    "x_train, x_test, word_index = prepare_tokenized_data((x_train, x_test), max_num_words, max_sequence_length)\n",
    "print('Finished tokenizing texts')\n",
    "print('-' * 80)\n",
    "embedding_matrix = load_embedding_matrix(embedding_path, word_index, embedding_dim,gensim=gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_tokenized_data(data, max_num_words, max_sequence_length):\n",
    "    train, test = data\n",
    "    if not os.path.exists('data/tokenizer.pkl'):\n",
    "        tokenizer = Tokenizer(num_words=max_num_words)\n",
    "        tokenizer.fit_on_texts(train)\n",
    "\n",
    "        with open('data/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "\n",
    "        print('Saved tokenizer.pkl')\n",
    "    else:\n",
    "        with open('data/tokenizer.pkl', 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "            print('Loaded tokenizer.pkl')\n",
    "\n",
    "    sequences_train = tokenizer.texts_to_sequences(train)\n",
    "    sequences_test = tokenizer.texts_to_sequences(test)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique 1-gram tokens.' % len(word_index))\n",
    "    print('Min sequence length: {}'.format(np.min(list(map(len, sequences_train)))))\n",
    "    print('Average sequence length: {}'.format(np.mean(list(map(len, sequences_train)), dtype=int)))\n",
    "    print('Max sequence length: {}'.format(np.max(list(map(len, sequences_train)))))\n",
    "    train = pad_sequences(sequences_train, maxlen=max_sequence_length)\n",
    "    test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "    return (train, test, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets :  100800 \n",
      "\n",
      "Number of negative tweets :  99322 \n",
      "\n",
      "indices : [     0      1      2 ..., 200119 200120 200121]\n",
      "Test split : 30.0 %\n",
      "Number of samples : 200122\n",
      "Train size : 140086\n",
      "Test size : 60036\n",
      "Tokenizing texts\n",
      "Saved tokenizer.pkl\n",
      "Found 112611 unique 1-gram tokens.\n",
      "Min sequence length: 2\n",
      "Average sequence length: 13\n",
      "Max sequence length: 49\n",
      "Finished tokenizing texts\n",
      "--------------------------------------------------------------------------------\n",
      "Load embedding model.\n",
      "Numbers of words in embedding model : 667062\n",
      "Preparing embedding matrix.\n",
      "Embeding matrix size : 112612\n",
      "Saved embedding matrix\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "max_num_words = 100000\n",
    "max_sequence_length = 30\n",
    "validation_split = 0.3\n",
    "embedding_path = './data/embeddings/wiki.ar.vec'\n",
    "embedding_path = './data/embeddings/wiki_corpus_tweets_xtrain_word2vec_size300_skip_gram_wv_iter=10'\n",
    "\n",
    "full_dataset = True\n",
    "x_train, y_train, x_test, y_test, word_index = prepare_data(max_num_words,max_sequence_length,validation_split,full_dataset)\n",
    "embedding_matrix = load_embedding_matrix(embedding_path, word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bidirectionelle GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model_bi_gru = Sequential()\n",
    "model_bi_gru.add(Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False))\n",
    "model_bi_gru.add(Bidirectional(GRU(300,activation='linear')))\n",
    "model_bi_gru.add(Activation('relu'))\n",
    "model_bi_gru.add(Dropout(0.5))\n",
    "model_bi_gru.add(Dense(64,activation='relu'))\n",
    "model_bi_gru.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "model_bi_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# checkpoint\n",
    "#bi_gru_filepath=\"./saved_models/ASTD.Bi.GRU.model-{val_acc:.2f}.hdf5\"\n",
    "bi_gru_filepath=\"./saved_models/DATA.Bi.GRU.model-{val_acc:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(bi_gru_filepath, monitor='val_acc', verbose=2, save_best_only=True, mode='max')\n",
    "tbCallBack = TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=False,embeddings_freq = 1 ,\n",
    "                                    embeddings_layer_names = None ,\n",
    "                                    embeddings_metadata = None)\n",
    "#history2 = model.fit(x_train,y_train,batch_size=100,epochs=10,validation_data=(x_test,y_test),verbose=1,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 962 samples, validate on 318 samples\n",
      "Epoch 1/3\n",
      "962/962 [==============================] - 7s - loss: 0.6812 - acc: 0.5696 - val_loss: 0.6518 - val_acc: 0.6981\n",
      "Epoch 2/3\n",
      "962/962 [==============================] - 6s - loss: 0.5670 - acc: 0.7339 - val_loss: 0.5430 - val_acc: 0.7327\n",
      "Epoch 3/3\n",
      "962/962 [==============================] - 7s - loss: 0.4455 - acc: 0.7890 - val_loss: 0.5129 - val_acc: 0.7547\n"
     ]
    }
   ],
   "source": [
    "history_bi_gru = model_bi_gru.fit(x_train,y_train,batch_size=100,epochs=3,\n",
    "                                  validation_data=(x_test,y_test),verbose=1,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60032/60036 [============================>.] - ETA: 0sAccuracy score :  0.88 \n",
      "\n",
      "Precision score :  0.87 \n",
      "\n",
      "Recall score:  0.90 \n",
      "\n",
      "F1 score :  0.89 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_bi_gru = load_model('./data/data_model/DATA.Bi.GRU.model-0.88.hdf5')\n",
    "y_pred = model_bi_gru.predict_classes(x_test)\n",
    "evaluate(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history =history_bi_gru\n",
    "print(history.history.keys())  \n",
    "\n",
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "# summarize history for loss  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(111)  \n",
    "plt.plot(history.history['loss'])  \n",
    "#plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['base d\\'entrainement', 'base de test'], loc='upper right')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
