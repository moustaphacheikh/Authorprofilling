{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Found C:\\Program Files\\Java\\jdk1.8.0_131\\bin\\java.exe: C:\\Program Files\\Java\\jdk1.8.0_131\\bin\\java.exe]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "import os\n",
    "from outils import *\n",
    "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131\\\\bin\\\\java.exe\"\n",
    "slf4j_path ='C:\\\\stanford-segmenter\\\\slf4j-api.jar'\n",
    "stanford_models_paths = 'C:\\\\stanford-segmenter\\\\data'\n",
    "classpath = 'C:\\\\stanford-segmenter\\\\slf4j-api.jar;C:\\\\stanford-segmenter\\\\stanford-segmenter.jar'\n",
    "nltk.internals.config_java(java_path)\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['SLF4J'] =slf4j_path\n",
    "os.environ['STANFORD_MODELS'] =stanford_models_paths\n",
    "os.environ['CLASSPATH'] = classpath\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_emojis(string):\n",
    "    pattern = re.compile(u'['\n",
    "                         u'\\U0001F300-\\U0001F64F'\n",
    "                         u'\\U0001F680-\\U0001F6FF'\n",
    "                         u'\\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    "    iterator = re.findall(pattern, string)\n",
    "    emojis = []\n",
    "    for emoji in iterator:\n",
    "        for m in emoji:\n",
    "            emojis.append(m)\n",
    "    return emojis\n",
    "def preprocessing(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'[A-Za-z]', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "def get_num_numbers(string):\n",
    "    regex = re.compile(r\"(\\d|[\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669])+\")\n",
    "    return len(re.findall(regex, string))\n",
    "def get_num_emojis(string):\n",
    "    return len(get_emojis(string))\n",
    "def get_spaces_ratio(string):\n",
    "    regex =re.compile(r\"\\s\")\n",
    "    num_epaces = len(re.findall(regex, string))\n",
    "    num_carracters= len(string)\n",
    "    if num_carracters>0:\n",
    "        return num_epaces/num_carracters\n",
    "    else:\n",
    "        return 0\n",
    "def get_numeric_ratio(string):\n",
    "    regex =re.compile(r\"\\d\")\n",
    "    num_numeric = len(re.findall(regex, string))\n",
    "    num_carracters= len(string)\n",
    "    if num_carracters>0:\n",
    "        return num_numeric/num_carracters\n",
    "    else:\n",
    "        return 0\n",
    "def get_max_length(string):\n",
    "    lengths= [len(s) for s in string.split()]\n",
    "    if len(lengths)>0:  \n",
    "        return max(lengths)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_min_length(string):\n",
    "    lengths= [len(s) for s in string.split()]\n",
    "    if len(lengths)>0:  \n",
    "        return min(lengths)\n",
    "    else:\n",
    "        return 0\n",
    "def get_num_words(string):\n",
    "    return len(string.split())\n",
    "\n",
    "def get_num_short_words(string):\n",
    "    short_words = [1 if len(s)<4 else 0 for s in string.split()]\n",
    "    return sum(short_words)\n",
    "def get_num_unique_words(string):\n",
    "    return len(set(string.split()))\n",
    "\n",
    "def get_num_stopwords(string):\n",
    "    stopwords = open('./data/stopwords.txt',encoding='utf8').read().split()\n",
    "    counter = [1 if word in stopwords else 0 for word in string.split()]\n",
    "    return sum(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = StanfordSegmenter()\n",
    "#tokenizer.default_config('ar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv('./data/train_data.csv',encoding='utf8')\n",
    "#train = train[['id','genre']].copy()\n",
    "df= pd.read_csv('./data/test_data.csv',encoding='utf8')\n",
    "#test = test[['id','genre']].copy()\n",
    "\n",
    "\n",
    "#df_train = get_tweets(train)\n",
    "df_test = get_tweets(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] =df_test.text.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'].replace('', np.nan, inplace=True)\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [string for string in df_test.text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "        \n",
    "    #sents= list(map(self.split, sentences))\n",
    "    return tokenizer.segment_sents(list(map(split, sentences))).splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def split(string):\n",
    "    return word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] =df.text.apply(preprocessing)\n",
    "df['num_emojis'] =df.text.apply(get_num_emojis)\n",
    "df['spaces_ratio'] =df.text.apply(get_spaces_ratio)\n",
    "df['numeric_ratio'] =df.text.apply(get_numeric_ratio)\n",
    "df['max_length'] =df.text.apply(get_max_length)\n",
    "df['min_length'] =df.text.apply(get_min_length)\n",
    "df['num_words'] =df.text.apply(get_num_words)\n",
    "df['num_short_words'] =df.text.apply(get_num_short_words)\n",
    "df['num_unique_words'] =df.text.apply(get_num_unique_words)\n",
    "df['num_carracters'] = df.text.apply(len)\n",
    "df['num_stopwords'] =df.text.apply(get_num_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label', 'etext', 'num_emojis', 'spaces_ratio', 'numeric_ratio',\n",
       "       'max_length', 'min_length', 'num_words', 'num_short_words',\n",
       "       'num_unique_words', 'num_carracters', 'num_stopwords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
