{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "import os\n",
    "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131\\\\bin\\\\java.exe\"\n",
    "slf4j_path ='C:\\\\stanford-segmenter\\\\slf4j-api.jar'\n",
    "stanford_models_paths = 'C:\\\\stanford-segmenter\\\\data'\n",
    "classpath = 'C:\\\\stanford-segmenter\\\\slf4j-api.jar;C:\\\\stanford-segmenter\\\\stanford-segmenter.jar'\n",
    "nltk.internals.config_java(java_path)\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['SLF4J'] =slf4j_path\n",
    "os.environ['STANFORD_MODELS'] =stanford_models_paths\n",
    "os.environ['CLASSPATH'] = classpath\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "def get_emojis(string):\n",
    "    pattern = re.compile(u'['\n",
    "                         u'\\U0001F300-\\U0001F64F'\n",
    "                         u'\\U0001F680-\\U0001F6FF'\n",
    "                         u'\\u2600-\\u26FF\\u2700-\\u27BF]+', re.UNICODE)\n",
    "    iterator = re.findall(pattern, string)\n",
    "    emojis = []\n",
    "    for emoji in iterator:\n",
    "        for m in emoji:\n",
    "            emojis.append(m)\n",
    "    return emojis\n",
    "def preprocessing(string):\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'[A-Za-z]', ' ', string)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", string).strip()\n",
    "def get_num_numbers(string):\n",
    "    regex = re.compile(r\"(\\d|[\\u0660\\u0661\\u0662\\u0663\\u0664\\u0665\\u0666\\u0667\\u0668\\u0669])+\")\n",
    "    return len(re.findall(regex, string))\n",
    "def get_num_emojis(string):\n",
    "    return len(get_emojis(string))\n",
    "def get_spaces_ratio(string):\n",
    "    regex =re.compile(r\"\\s\")\n",
    "    num_epaces = len(re.findall(regex, string))\n",
    "    num_carracters= len(string)\n",
    "    return num_epaces/num_carracters\n",
    "def get_numeric_ratio(string):\n",
    "    regex =re.compile(r\"\\d\")\n",
    "    num_numeric = len(re.findall(regex, string))\n",
    "    num_carracters= len(string)\n",
    "    return num_numeric/num_carracters\n",
    "def get_max_length(string):\n",
    "    lengths= [len(s) for s in string.split()]\n",
    "    return max(lengths)\n",
    "\n",
    "def get_min_length(string):\n",
    "    lengths= [len(s) for s in string.split()]\n",
    "    return min(lengths)\n",
    "def get_num_words(string):\n",
    "    return len(string.split())\n",
    "\n",
    "def get_num_short_words(string):\n",
    "    short_words = [1 if len(s)<4 else 0 for s in string.split()]\n",
    "    return sum(short_words)\n",
    "def get_num_unique_words(string):\n",
    "    return len(set(string.split()))\n",
    "\n",
    "def get_num_stopwords(string):\n",
    "    stopwords = open('./data/stopwords.txt',encoding='utf8').read().split()\n",
    "    counter = [1 if word in stopwords else 0 for word in string.split()]\n",
    "    return sum(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = StanfordSegmenter()\n",
    "tokenizer.default_config('ar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def tokenize(string):\n",
    "    tokens = string.split()\n",
    "    sentence_tekonizeds = tokenizer.segment(tokens)\n",
    "    return re.sub(r'\\s+', ' ', sentence_tekonizeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "file_name ='./data/dataexcel.xlsx'\n",
    "df = pd.read_excel(file_name)\n",
    "df = df[['sexe','comment']].copy()\n",
    "df['text'] =df.comment.apply(preprocessing)\n",
    "df['text'].replace('', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [string.split() for string in df.text.values]\n",
    "sentences = tokenizer.segment_sents(sentences).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentencess = [string for string in sentences if len(string)>0 ]\n",
    "s1 = pd.DataFrame(sentencess,columns=['text'])\n",
    "df = df.assign(etext=s1.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['etext'] =df.comment.apply(preprocessing)\n",
    "df['sexe'].replace('f', 'F', inplace=True)\n",
    "df['num_emojis'] =df.comment.apply(get_num_emojis)\n",
    "df['spaces_ratio'] =df.comment.apply(get_spaces_ratio)\n",
    "df['numeric_ratio'] =df.comment.apply(get_numeric_ratio)\n",
    "df['max_length'] =df.comment.apply(get_max_length)\n",
    "df['min_length'] =df.comment.apply(get_min_length)\n",
    "df['num_words'] =df.comment.apply(get_num_words)\n",
    "df['num_short_words'] =df.comment.apply(get_num_short_words)\n",
    "df['num_unique_words'] =df.comment.apply(get_num_unique_words)\n",
    "df['num_carracters'] = df.comment.apply(len)\n",
    "df['num_stopwords'] =df.comment.apply(get_num_stopwords)\n",
    "#df.columns=['sexe','comment','text','etext','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']\n",
    "#df_norm = df[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']].copy()\n",
    "#df.sexe.value_counts()\n",
    "\n",
    "#df[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']] = (df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']] - df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].mean()) / (df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].max() - df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']] = (df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']] - df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].mean()) / (df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].max() - df[['num_emojis','spaces_ratio','numeric_ratio','max_length','min_length','num_words','num_short_words','num_unique_words','num_carracters','num_stopwords']].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131\\\\bin\\\\java.exe\"\n",
    "slf4j_path ='C:\\\\stanford-segmenter\\\\slf4j-api.jar'\n",
    "stanford_models_paths = 'C:\\\\stanford-postagger\\\\models'\n",
    "classpath = 'C:\\\\stanford-postagger\\\\stanford-postagger.jar'\n",
    "nltk.internals.config_java(java_path)\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['SLF4J'] =slf4j_path\n",
    "os.environ['STANFORD_MODELS'] =stanford_models_paths\n",
    "os.environ['CLASSPATH'] = classpath\n",
    "arabic_tager ='C:\\stanford-postagger\\\\models\\\\arabic.tagger'\n",
    "\n",
    "def get_num_of_pos(pos,string):\n",
    "    return len(re.findall(pos,string))\n",
    "#list(set(re.findall(r'[A-Z]+',string)))\n",
    "POSs=['VBN', 'DTJJ', 'WP', 'JJR', 'NNS', 'VBG', 'RB', 'DTNN', 'VN', 'PUNC', 'DTJJR', 'NNP', 'UH', 'NN', 'VBD', 'DTNNPS', 'CC', 'DT', 'IN', 'NOUN', 'VB', 'CD', 'DTNNS', 'RP', 'PRP', 'VBP', 'WRB', 'ADJ', 'DTNNP', 'JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = StanfordPOSTagger(arabic_tager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(string):\n",
    "    return ' '.join(reversed([w[1] for w in string]))\n",
    "def tag_sents(sentences):\n",
    "    return [tag(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences =[s.split() for s in df.etext.values]\n",
    "sentences_tags = tagger.tag_sents(sentences)\n",
    "sents = tag_sents(sentences_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame(sents,columns=['pos_text'])\n",
    "df = df.assign(pos_text=s1.pos_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pos in POSs:\n",
    "    df[pos]=0\n",
    "for pos in POSs:\n",
    "    for index,row in df.iterrows():\n",
    "        df.loc[index,pos]= get_num_of_pos(pos,row.pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_json('df.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('df.json',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ADJ', 'CC', 'CD', 'DT', 'DTJJ', 'DTJJR', 'DTNN', 'DTNNP', 'DTNNPS',\n",
       "       'DTNNS', 'IN', 'JJ', 'JJR', 'NN', 'NNP', 'NNS', 'NOUN', 'PRP', 'PUNC',\n",
       "       'RB', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VN', 'WP', 'WRB',\n",
       "       'comment', 'etext', 'max_length', 'min_length', 'num_carracters',\n",
       "       'num_emojis', 'num_short_words', 'num_stopwords', 'num_unique_words',\n",
       "       'num_words', 'numeric_ratio', 'pos_text', 'sexe', 'spaces_ratio',\n",
       "       'text', 'x1', 'x10', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
